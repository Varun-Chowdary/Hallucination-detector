{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318083c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Obtaining dependency information for sentence_transformers from https://files.pythonhosted.org/packages/76/2c/bd95032aeb087b0706596af0a4518c4bfe0439a1bb149048ece18b617766/sentence_transformers-2.7.0-py3-none-any.whl.metadata\n",
      "  Using cached sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/89/a9/8e097f79d2941a2f96e33f57032957429a79f66c8252ac7fcce586a43406/datasets-2.19.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/fb/12/2f5c8d4764b00033cf1c935b702d3bb878d10be9f0b87f0253495832d85f/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.34.0 from https://files.pythonhosted.org/packages/09/c8/844d5518a6aeb4ffdc0cf0cae65ae13dbe5838306728c5c640b5a6e2a0c9/transformers-4.40.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (2.0.0+cu117)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.11.2)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence_transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.15.1 from https://files.pythonhosted.org/packages/05/c0/779afbad8e75565c09ffa24a88b5dd7e293c92b74eb09df6435fc58ac986/huggingface_hub-0.22.2-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (10.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.12.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/eb/3a/25c4aecb61a49d4415fd71d4f66a8a5b558dd44a52d7054ea9aa59ccbac1/xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2023.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers) (3.27.6)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.11.0->sentence_transformers) (17.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/a7/03/fb50fc03f86016b227a967c8d474f90230c885c0d18f78acdfda7a96ce56/tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/d5/85/1e7d2804cbf82204cde462d16f1cb0ff5814b03f559fb46ceaa6b7020db4/safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.9,>=0.3.0 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "Using cached datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Using cached transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: sentencepiece, xxhash, safetensors, pyarrow-hotfix, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets, sentence_transformers\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 pyarrow-hotfix-0.6 safetensors-0.4.3 sentence_transformers-2.7.0 sentencepiece-0.2.0 tokenizers-0.19.1 transformers-4.40.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f936757-5270-4080-af38-e96a3e5cda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1662ec7d-c077-4502-9d0d-27f83ab9951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6922d1-ddae-4628-bd3d-1c398085ea38",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872d4292-f9dc-45c1-932e-257bae6f22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the SNLI dataset\n",
    "dataset = load_dataset(\"paws\", \"labeled_final\")\n",
    "dataset = dataset.filter(lambda x: x['label'] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee29d380-b02f-41f4-9f1e-f12a69b71e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c4c90b-9583-4b63-b76b-9c330de0fde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'sentence1': 'The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .',\n",
       " 'sentence2': 'The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53b91ba-2d10-4dbe-beee-08f525687b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          sentence1  \\\n",
      "0   1  In Paris , in October 1560 , he secretly met t...   \n",
      "1   2  The NBA season of 1975 -- 76 was the 30th seas...   \n",
      "2   3  There are also specific discussions , public p...   \n",
      "3   4  When comparable rates of flow can be maintaine...   \n",
      "4   5  It is the seat of Zerendi District in Akmola R...   \n",
      "5   6  William Henry Henry Harman was born on 17 Febr...   \n",
      "6   7  Bullion Express - concept is being introduced ...   \n",
      "7   8  With a discrete amount of probabilities Formul...   \n",
      "8   9  The Soviet Union maintained an embassy in Oslo...   \n",
      "9  10  Vocabulary even went to Brazil through leaving...   \n",
      "\n",
      "                                           sentence2  label  \n",
      "0  In October 1560 , he secretly met with the Eng...      0  \n",
      "1  The 1975 -- 76 season of the National Basketba...      1  \n",
      "2  There are also public discussions , profile sp...      0  \n",
      "3  The results are high when comparable flow rate...      1  \n",
      "4  It is the seat of the district of Zerendi in A...      1  \n",
      "5  William Henry Harman was born in Waynesboro , ...      1  \n",
      "6  2011-DGSE Bullion Express concept is introduce...      0  \n",
      "7  Given a discrete set of probabilities formula ...      1  \n",
      "8  The Soviet Union maintained an embassy in Mosc...      0  \n",
      "9  Vocabulary even went to Brazil by leaving Maca...      0  \n"
     ]
    }
   ],
   "source": [
    "# Print first row from train set\n",
    "print(df_train.iloc[0:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0858d8-f652-45f7-87c6-7837c75d5e53",
   "metadata": {},
   "source": [
    "## Convert dataset to list of InputExamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b28f3ea-ad46-4fbc-b37c-24162bdeba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training data: [0 1]\n",
      "Unique labels in testing data: [0 1]\n"
     ]
    }
   ],
   "source": [
    "unique_train_labels = df_train['label'].unique()\n",
    "unique_test_labels = df_test['label'].unique()\n",
    "\n",
    "print(\"Unique labels in training data:\", unique_train_labels)\n",
    "print(\"Unique labels in testing data:\", unique_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4758a7-1eac-477d-a3d1-237c21ec3805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b907d6d-a2c6-4818-bac4-c84bed858010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=int(row['label'])) for index, row in df_train.iterrows()]\n",
    "test_examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=int(row['label'])) for index, row in df_test.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc71a94-9bc6-4222-b198-cac27f277b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_evaluator = CEBinaryClassificationEvaluator.from_input_examples(test_examples, name='test_eval')\n",
    "except AssertionError as e:\n",
    "    print(\"Error with labels:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c862d2-5f86-48dc-b112-73da4d50c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<InputExample> label: 0, texts: In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .; In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .\n"
     ]
    }
   ],
   "source": [
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6e1cc0-aa5b-4316-8351-df38ac97ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator = CEBinaryClassificationEvaluator.from_input_examples(test_examples, name='test_eval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4724b757-c905-4b8e-862c-75717d09653a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49401"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6803bee0-4090-493e-8fad-8f123cd7b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 22 03:46:32 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:C4:00.0 Off |                  Off |\n",
      "| 30%   37C    P8             33W /  300W |       0MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1426b22-f7d9-4dce-ac5a-96a5344256f9",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27c7e80-23d6-47d9-900e-c17740cb77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at cross-encoder/nli-deberta-v3-base and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model_save_path = \"./model_dump\"\n",
    "model_name = 'cross-encoder/nli-deberta-v3-base' # base model, use 'vectara/hallucination_evaluation_model' if you want to further fine-tune ours\n",
    "\n",
    "model = CrossEncoder(model_name, num_labels=1, automodel_args={'ignore_mismatched_sizes':True})\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4334e36c-9591-4260-b6e6-b34becac6ad2",
   "metadata": {},
   "source": [
    "train_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afdedc-6916-4249-84a7-3a9fc94b8859",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96d1b1b-4ad9-4a6e-a8a3-d74469d59997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6808c7dc974c2b8b558f9407881c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f1c76603c450bb40f949d5a7e1d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb9b0ba5b974ed0a404f73b4fa24eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ce4e56bfc84cf9b235be946ebdc775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3855532521ac4c379083b70b99e7a2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ede923007441d18f756726c4156b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3398adb4bc2e4f949715afbafc3344e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f538c4438a4b74b5f7b1e5920d3ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c73f6c511824627a37a0f20d3cbdf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83eedec79e241e1a0fd6f2666000e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90c561ce4834465bf47e36f35f92aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=128)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=test_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=10_000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734053af-d55d-4b81-9d77-3249ec6b51b3",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac222112-4de9-497e-9f81-713605e263ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51441849-c05e-43cd-84a2-70cb59db9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"./model_output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bb08a40-5150-4095-b8d0-181a47d883c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977efc18-b0a6-4d7a-9718-af3e22fc984c",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6677eed-5d66-4771-9361-6133dab34328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# Load the model from the path\n",
    "model = CrossEncoder(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba54ab6d-a261-4b1b-bfa1-778497e13e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(model, sentence1, sentence2):\n",
    "    # Directly use the predict method without setting eval\n",
    "    with torch.no_grad():  # It's still good practice to use no_grad() to disable gradient calculation\n",
    "        predictions = model.predict([(sentence1, sentence2)])\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845c98e5-ee46-4a76-a37f-2d9d1827856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.8128992\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Assuming the model is already loaded and available as `model`\n",
    "sentence1 = \"The company reported an overall revenue growth of 14.3%. This increase is attributed to a surge in demand for IT services, successful acquisitions, new deal wins including large deals, and the depreciation of the Indian Rupee against foreign currencies like the USD and Canadian Dollar.\"\n",
    "sentence2 = \"The overall revenue growth percentage for the company in the reported year was 14.3%.\"\n",
    "\n",
    "prediction = predict_with_model(model, sentence1, sentence2)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8caf37-792d-453e-96de-0a09d242d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.00411069]\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "# Assuming the model is already loaded and available as `model`\n",
    "sentence1 = \"The profit generated in 2022 is 24 million dollars\"\n",
    "sentence2 = \"A profit generated in 2022 is $25.00 million\"\n",
    "prediction = predict_with_model(model, sentence1, sentence2)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15b8c3ef-3050-46af-a4ef-8d763f0e3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model from the saved directory\n",
    "model = CrossEncoder(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e1c084-14c2-464d-b59e-d6eb57b60035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Score: [0.98758197]\n"
     ]
    }
   ],
   "source": [
    "def predict_with_model(model, sentence1, sentence2):\n",
    "    # Use the model's predict method which expects a list of sentence pairs\n",
    "    predictions = model.predict([(sentence1, sentence2)])\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"Einstein is from Germany.\"\n",
    "sentence2 = \"Einstein is from Berlin, Gemany.\"\n",
    "prediction = predict_with_model(model, sentence1, sentence2)\n",
    "print(\"Prediction Score:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efcc70-cde0-4e6f-a174-e60bed566a77",
   "metadata": {},
   "source": [
    "## Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97788ad9-f2bd-446f-b063-645772faf53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4171f15-8638-4fcf-ac5e-c6980fb6d6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3939cc-5fd7-4d14-a5a9-bf5fc9768756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ad367be-7015-43eb-b7fc-eb31e7cd04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivate gradients for the following code\n",
    "        for texts, labels in dataloader:\n",
    "            predictions = model.predict(texts)  # This assumes that 'predict' returns a list or array of probabilities\n",
    "            \n",
    "            # Convert predictions to a tensor if not already one and apply threshold\n",
    "            predictions_tensor = torch.tensor(predictions)\n",
    "            predicted_labels = (predictions_tensor >= 0.5).long()  # Classify predictions based on the 0.5 threshold\n",
    "            #import pdb;pdb.set_trace()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e22a1f4-bcb3-48d5-8354-9f1c8edb1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    texts = [[ex.texts[0], ex.texts[1]] for ex in batch]  # Extract texts from each InputExample\n",
    "    labels = torch.tensor([ex.label for ex in batch])  # Extract labels and convert to tensor\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_examples, batch_size=128, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ad0362a-f626-44d9-873c-84173662d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.94875\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = calculate_accuracy(model, test_dataloader)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18119e93-13b0-4212-984b-e8534eeb0183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This was a series of nested angular standards , so that measurements in azimuth and elevation could be done directly in polar coordinates relative to the ecliptic .',\n",
       " 'This was a series of nested polar scales , so that measurements in azimuth and elevation could be performed directly in angular coordinates relative to the ecliptic .']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples[0].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c571049-3bc0-45f6-85a8-60a7009b4acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d73f657-6027-439a-be21-de2c53bdff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Obtaining dependency information for torchmetrics from https://files.pythonhosted.org/packages/f3/0e/cedcb9c8aeb2d1f655f8d05f841b14d84b0a68d9f31afae4af55c7c6d0a9/torchmetrics-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (1.24.3)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (23.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (2.0.0+cu117)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/5e/9e/e7768a8e363fc6f0c978bb7a0aa7641f10d80be60000e788ef2f01d34a7c/lightning_utilities-0.11.2-py3-none-any.whl.metadata\n",
      "  Using cached lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.1.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.12.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (3.27.6)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (17.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Using cached torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "Using cached lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.2 torchmetrics-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66c26592-b827-440a-8a89-44328d2a58cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94875\n",
      "Precision: 0.9282191780821918\n",
      "Recall: 0.9581447963800905\n",
      "F1 Score: 0.9429446145282494\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_metrics(model, dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivate gradients for the following code\n",
    "        for texts, labels in dataloader:\n",
    "            predictions = model.predict(texts)  # Assumes 'predict' returns a list or array of probabilities\n",
    "            \n",
    "            predictions_tensor = torch.tensor(predictions)\n",
    "            predicted_labels = (predictions_tensor >= 0.5).long()  # Classify predictions based on the 0.5 threshold\n",
    "            \n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Calculate true positives, false positives, and false negatives\n",
    "            true_positives += ((predicted_labels == 1) & (labels == 1)).sum().item()\n",
    "            false_positives += ((predicted_labels == 1) & (labels == 0)).sum().item()\n",
    "            false_negatives += ((predicted_labels == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Example usage assuming the dataloader and model are set up:\n",
    "accuracy, precision, recall, f1 = calculate_metrics(model, test_dataloader)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a24c61-1272-4e47-b28d-1101d3eeaabd",
   "metadata": {},
   "source": [
    "## Uploading it on Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46ca6603-ec61-40dc-85cd-2a9d9338c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (0.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2023.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11149f28-6020-4541-b29b-489767105c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): "
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71cbadb-deb0-40d5-abe6-c310248559d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, HfFolder\n",
    "\n",
    "# Define paths and repository details\n",
    "local_dir = 'training'  # Local directory where your model and README.md are saved\n",
    "repo_name = 'hallucination_detector'  # Repository name on Hugging Face Hub\n",
    "username = 'Varun-Sayapaneni'  # Your Hugging Face username\n",
    "\n",
    "# Create or use existing repository\n",
    "repo = Repository(local_dir=local_dir, clone_from=f'{username}/{repo_name}', use_auth_token=True)\n",
    "\n",
    "# If you need to add files, you can do it here\n",
    "# For example:\n",
    "# shutil.copy('my_model.bin', local_dir)\n",
    "\n",
    "# Commit and push files to the repository\n",
    "repo.git_add('*')\n",
    "repo.git_commit(\"Initial model upload\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcfee6-8eed-42cc-8703-8b21d26f8583",
   "metadata": {},
   "source": [
    "## Storing values to our prompt database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41fcf9c-e80a-4368-ac81-f90110738e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv('prompt_database_csv.csv')\n",
    "\n",
    "# Clean up column names by stripping any extra spaces\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Load model\n",
    "model_save_path = \"./model_output\"\n",
    "model = CrossEncoder(model_save_path)\n",
    "\n",
    "def predict_with_model(model, sentence1, sentence2):\n",
    "    with torch.no_grad():\n",
    "        predictions = model.predict([(sentence1, sentence2)])\n",
    "    return predictions[0]\n",
    "\n",
    "# Process rows and store predictions\n",
    "for i in range(min(10, len(data))):\n",
    "    sentence1 = data.loc[i, 'Output'].strip()\n",
    "    sentence2 = data.loc[i, 'Answer'].strip()  # Fixed column name reference here\n",
    "    prediction = predict_with_model(model, sentence1, sentence2)\n",
    "    data.loc[i, 'DeBERTa'] = prediction\n",
    "\n",
    "# Save the results back to CSV\n",
    "data.to_csv('prompt_database_csv.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b00043-7277-404c-acac-ffd2f9873880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Prompt  \\\n",
      "0  What was the overall revenue growth percentage...   \n",
      "1  How much did the revenue of the IT Services se...   \n",
      "2  What factors contributed to the 2.0% decline i...   \n",
      "3  By what percentage did revenue from the ISRE s...   \n",
      "4  What strategic change was announced effective ...   \n",
      "5  How did the selling and marketing expenses cha...   \n",
      "6  What were the primary reasons for the 27.5% in...   \n",
      "7  How did the operating income and operating mar...   \n",
      "8  How did the company's finance and other income...   \n",
      "9  What was the increase in income taxes from FY ...   \n",
      "\n",
      "                                              Answer  \\\n",
      "0  The company reported an overall revenue growth...   \n",
      "1  The revenue for the IT Services segment saw an...   \n",
      "2  The IT Products segment experienced a 2.0% rev...   \n",
      "3  The ISRE segment's revenue declined by 20.2%, ...   \n",
      "4  Effective April 1, 2023, the company announced...   \n",
      "5  Selling and marketing expenses as a percentage...   \n",
      "6  The general and administrative expenses saw a ...   \n",
      "7  The operating income decreased marginally by 0...   \n",
      "8  The finance and other income increased from B1...   \n",
      "9  Income taxes increased by B5,046 million, from...   \n",
      "\n",
      "                                             Context Organization Prompt.1  \\\n",
      "0  Revenue\\nOur revenue increased by 14.3%. Our I...       Wipro       NaN   \n",
      "1                                                NaN       Wipro       NaN   \n",
      "2                                                NaN       Wipro       NaN   \n",
      "3                                                NaN       Wipro       NaN   \n",
      "4                                                NaN       Wipro       NaN   \n",
      "5                                                NaN       Wipro       NaN   \n",
      "6                                                NaN       Wipro       NaN   \n",
      "7                                                NaN       Wipro       NaN   \n",
      "8                                                NaN       Wipro       NaN   \n",
      "9                                                NaN       Wipro       NaN   \n",
      "\n",
      "                                              Output   DeBERTa  FAVA  \n",
      "0  The overall revenue growth percentage for the ...  0.505386   NaN  \n",
      "1  The revenue of the IT Services segment increas...  0.839491   NaN  \n",
      "2  The 2.0% decline in IT Products segment revenu...  0.951768   NaN  \n",
      "3  Revenue from the ISRE segment declined by 20.2...  0.414557   NaN  \n",
      "4  Effective April 1, 2023, the strategic change ...  0.929614   NaN  \n",
      "5  The selling and marketing expenses as a percen...  0.182075   NaN  \n",
      "6  The 27.5% increase in general and administrati...  0.643743   NaN  \n",
      "7  The operating income decreased marginally by 0...  0.612929   NaN  \n",
      "8  The company's finance and other income increas...  0.736812   NaN  \n",
      "9  The increase in income taxes from FY 2022 to F...  0.633608   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Read back the CSV to confirm updates\n",
    "updated_data = pd.read_csv('prompt_database_csv.csv')\n",
    "print(updated_data.head(10))  # Print the first 10 rows of the updated CSV to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c3bb2-0520-49d8-932a-a2e10fe909cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv('prompt_database_csv.csv')\n",
    "# Clean up column names by stripping any extra spaces\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Load model\n",
    "model_save_path = \"./model_output\"\n",
    "model = CrossEncoder(model_save_path)\n",
    "\n",
    "def predict_with_model(model, sentence1, sentence2):\n",
    "    with torch.no_grad():\n",
    "        predictions = model.predict([(sentence1, sentence2)])\n",
    "    return predictions[0]\n",
    "\n",
    "# Process rows and store predictions, starting from the second row (index 1)\n",
    "for i in range(1, len(data)):  # Start from 1 to skip the header row\n",
    "    # Convert entries to string and strip spaces\n",
    "    sentence1 = str(data.loc[i, 'Output']).strip()\n",
    "    sentence2 = str(data.loc[i, 'Answer']).strip()\n",
    "    \n",
    "    # Check if either sentence is empty after stripping\n",
    "    if sentence1 == \"\" or sentence2 == \"\":\n",
    "        print(f\"Skipping row {i} due to missing data.\")\n",
    "        continue\n",
    "\n",
    "    prediction = predict_with_model(model, sentence1, sentence2)\n",
    "    data.loc[i, 'DeBERTa'] = prediction  # Ensure column 'DeBERTa' exists or is properly named\n",
    "    # Print the output to console (optional, can be removed for large datasets)\n",
    "    print(f'Row {i}: Prediction = {prediction}')\n",
    "\n",
    "# Save the results back to CSV starting from row 2\n",
    "data.iloc[1:].to_csv('prompt_database_csv.csv', index=False)\n",
    "\n",
    "# Optionally, confirm some of the results are saved correctly (comment out for large datasets)\n",
    "print(data.iloc[1:11])  # Print the first 10 rows of the updated data starting from row 2 to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec11d4d-934d-46a2-95a3-07071c1aef5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
